{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment3_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanLeyva/DeepLearning/blob/main/Assignment3_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr06uR9YTUvq"
      },
      "source": [
        "# Assignment 3: Attention\n",
        "\n",
        "The objectives of this assignment are:\n",
        "\n",
        "+ To implement Bahdanau Attention and Luong General Attention classes.\n",
        "+ To do a comparative (# of steps to converge, test error) of the three methods we have seen. Use these values for the comparative (the training datset size and `rnn_units` and `batch_size` values are up to you): \n",
        "    + `n_timesteps_in = 100`\n",
        "    + `n_features = 20`.   \n",
        "+ To implement a function to visualize the attention weights for one example. You can visualize them as in this figure (that corresponds to a machine translation task):\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://jalammar.github.io/images/attention_sentence.png\" width=\"200\">\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "+ To write a blog entry explaining in your words how does attention work. You can do it in your favourite blog site. If you do not have a favourite blog site, you can start one here: https://hackmd.io/\n",
        "\n",
        "You have to report all your work at the end of this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCzOe5TW3pT9"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMIL6Mz-SaB_"
      },
      "source": [
        "#@title Some utils\n",
        "from random import randint\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_sequence(length, n_unique):\n",
        "    \"\"\"\n",
        "    Generate a sequence of random integers.\n",
        "    \n",
        "    :length: Total length of the generated sequence\n",
        "    :n_unique: Maximum number allowed\n",
        "    \"\"\"\n",
        "    return [randint(1, n_unique-1) for _ in range(length)]\n",
        "\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "    \"\"\"\n",
        "    Transform a sequence of integers into a one-hot-encoding vector\n",
        "    \n",
        "    :sequence: The sequence we want to transform\n",
        "    :n_unique: Maximum number allowed (length of the one-hot-encoded vector)\n",
        "    \"\"\"\n",
        "    encoding = list()\n",
        "    for value in sequence:\n",
        "        vector = [0 for _ in range(n_unique)]\n",
        "        vector[value] = 1\n",
        "        encoding.append(vector)\n",
        "    return np.array(encoding)\n",
        "\n",
        "def one_hot_decode(encoded_seq):\n",
        "    \"\"\"\n",
        "    Transorm a one-hot-encoded vector into a list of integers\n",
        "    \n",
        "    :encoded_seq: One hot encoded sequence to be transformed\n",
        "    \"\"\"\n",
        "    return [np.argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "\n",
        "def get_reversed_pairs(time_steps,vocabulary_size):\n",
        "    \"\"\"\n",
        "    Generate a pair X, y where y is the 'reversed' version of X.\n",
        "    \n",
        "    :time_steps: Sequence length\n",
        "    :vocabulary_size: Maximum number allowed\n",
        "    \"\"\"\n",
        "    # generate random sequence and reverse it\n",
        "    sequence_in = generate_sequence(time_steps, vocabulary_size)\n",
        "    sequence_out = sequence_in[::-1]\n",
        "\n",
        "    # one hot encode both sequences\n",
        "    X = one_hot_encode(sequence_in, vocabulary_size)\n",
        "    y = one_hot_encode(sequence_out, vocabulary_size)\n",
        "    \n",
        "    # reshape as 3D so it can be inputed to the LSTM\n",
        "    X = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "    y = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "    return X,y\n",
        "\n",
        "\n",
        "def create_dataset(train_size, test_size, time_steps,vocabulary_size):\n",
        "    \"\"\"\n",
        "    Generates a datset of reversed pairs X, y.\n",
        "    \n",
        "    :train_size: Number of train pairs\n",
        "    :test_size: Number of test pairs\n",
        "    :time_steps: Sequence length\n",
        "    :vocabulary_size: Maximum number allowed\n",
        "    \"\"\"\n",
        "    \n",
        "    # Generate reversed pairs for training\n",
        "    pairs = [get_reversed_pairs(time_steps,vocabulary_size) for _ in range(train_size)]\n",
        "    pairs= np.array(pairs).squeeze()\n",
        "    X_train = pairs[:,0]\n",
        "    y_train = pairs[:,1]\n",
        "    \n",
        "    # Generate reversed pairs for test\n",
        "    pairs = [get_reversed_pairs(time_steps,vocabulary_size) for _ in range(test_size)]\n",
        "    pairs= np.array(pairs).squeeze()\n",
        "    X_test = pairs[:,0]\n",
        "    y_test = pairs[:,1]\t\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def train_test(model, X_train, y_train , X_test, y_test, epochs=500, batch_size=32, patience=5):\n",
        "    \"\"\"\n",
        "    It trains a model and evaluates the result on the test dataset\n",
        "    \n",
        "    :model: Model to be fit\n",
        "    :X_train, y_train: Train samples and labels \n",
        "    :X_test y_test: Test samples and labels \n",
        "    :epochs: Maximum number of iterations that the model will perform\n",
        "    :batch_size: Samples per batch\n",
        "    :patience: Number of rounds without improvement that the model can perform. If there is no improvement on the loss, it will stop the trainning process.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Train the model\n",
        "    history=model.fit(X_train, y_train, \n",
        "                      validation_split= 0.1, \n",
        "                      epochs=epochs,\n",
        "                      batch_size=batch_size, \n",
        "                      callbacks=[EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)])\n",
        "    \n",
        "    _, train_acc = model.evaluate(X_train, y_train, batch_size=batch_size)\n",
        "    _, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "    \n",
        "    print('\\nPREDICTION ACCURACY (%):')\n",
        "    print('Train: %.3f, Test: %.3f' % (train_acc*100, test_acc*100))\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
        "    # summarize history for loss\n",
        "    axs[0].plot(history.history['loss'])\n",
        "    axs[0].plot(history.history['val_loss'])\n",
        "    axs[0].set_title(model.name+' loss')\n",
        "    axs[0].set_ylabel('loss')\n",
        "    axs[0].set_xlabel('epoch')\n",
        "    axs[0].legend(['train', 'val'], loc='upper left')\n",
        "    \n",
        "    # summarize history for accuracy\n",
        "    axs[1].plot(history.history['accuracy'])\n",
        "    axs[1].plot(history.history['val_accuracy'])\n",
        "    axs[1].set_title(model.name+' accuracy')\n",
        "    axs[1].set_ylabel('accuracy')\n",
        "    axs[1].set_xlabel('epoch')\n",
        "    axs[1].legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "def predict(model, n_timesteps_in,n_features, x, y_real=None, ):\n",
        "    pred=model.predict(x.reshape(1,n_timesteps_in,n_features), batch_size=1)\n",
        "    print('input', one_hot_decode(x))    \n",
        "    print('predicted', one_hot_decode(pred[0]))\n",
        "    if y_real is not None:\n",
        "        print('expected', one_hot_decode(y_real))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nOz2xOeRfWv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "#attention model\n",
        "def build_attention_model(attention, batch_size, rnn_units):\n",
        "    \n",
        "    # ENCODER STEP\n",
        "    # ------------\n",
        "    # Same encoder as before with one and only difference. Now we need all the lstm states so we\n",
        "    # set return_sequences=True and return_state=True.\n",
        "    encoder_inputs = Input(shape=(n_timesteps_in, n_features), name='encoder_inputs')\n",
        "    encoder_lstm = LSTM(rnn_units, return_sequences=True, return_state=True, name='encoder_lstm')\n",
        "    encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "    \n",
        "    states = [encoder_state_h, encoder_state_c]\n",
        "    \n",
        "    # DECODER STEP\n",
        "    # ------------\n",
        "    # Set up the decoder layers\n",
        "    # input shape: (1, n_features + rnn_units)\n",
        "    decoder_lstm = LSTM(rnn_units, return_state=True, name='decoder_lstm')\n",
        "    decoder_dense = Dense(n_features, activation='softmax', name='decoder_dense')\n",
        "    \n",
        "    # As before, we use as first input the 0-sequence\n",
        "    all_outputs = []\n",
        "    inputs = np.zeros((batch_size, 1, n_features))\n",
        "    \n",
        "    # Decoder_outputs is the last hidden state of the encoder. Encoder_outputs are all the states\n",
        "    decoder_outputs = encoder_state_h\n",
        "    \n",
        "    # Decoder will only process one time step at a time.\n",
        "    for _ in range(n_timesteps_in):\n",
        "\n",
        "        # Pay attention!\n",
        "        # decoder_outputs (last hidden state) + encoder_outputs (all hidden states)\n",
        "        context_vector, attention_weights = attention(decoder_outputs, encoder_outputs)\n",
        "        context_vector = tf.expand_dims(context_vector, 1)\n",
        "\n",
        "        # create the context vector by applying attention to \n",
        "        # Concatenate the input + context vectore to find the next decoder's input\n",
        "        inputs = tf.concat([context_vector, inputs], axis=-1)\n",
        "\n",
        "        # Passing the concatenated vector to the LSTM\n",
        "        # Run the decoder on one timestep with attended input and previous states\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(inputs, initial_state=states)        \n",
        "        outputs = decoder_dense(decoder_outputs)\n",
        "        \n",
        "        # Use the last hidden state for prediction the output\n",
        "        # save the current prediction\n",
        "        # we will concatenate all predictions later\n",
        "        outputs = tf.expand_dims(outputs, 1)\n",
        "        all_outputs.append(outputs)\n",
        "        \n",
        "        # Reinject the output (prediction) as inputs for the next loop iteration\n",
        "        # as well as update the states\n",
        "        inputs = outputs\n",
        "        states = [state_h, state_c]\n",
        "        \n",
        "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "    model = Model(encoder_inputs, decoder_outputs, name='model_encoder_decoder')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "class LuongDotAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(LuongDotAttention, self).__init__()\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
        "\n",
        "        # LUONGH Dot-product\n",
        "        score = tf.transpose(tf.matmul(query_with_time_axis, \n",
        "                                       values_transposed), perm=[0, 2, 1])\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lutvtkrYTR7r"
      },
      "source": [
        "# dataset \n",
        "n_timesteps_in = 100  # Sequence length\n",
        "n_features = 20     # Maximum number allowed-1 (length of the one-hot-encoded vector)\n",
        "train_size = 2000 \n",
        "test_size = 200\n",
        "X_train, y_train, X_test, y_test = create_dataset(train_size, test_size, n_timesteps_in,n_features )\n",
        "\n",
        "# training parameters\n",
        "batch_size = 100\n",
        "\n",
        "# model parameters\n",
        "rnn_units = 100\n",
        "\n",
        "# attention model\n",
        "attention = LuongDotAttention()\n",
        "model_attention = build_attention_model(attention, batch_size, rnn_units)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AggCVRldXsW7",
        "outputId": "ca303048-ead6-4ea8-d6d0-a680d97a2cb8"
      },
      "source": [
        "#training\n",
        "train_test(model_attention, X_train, y_train , X_test,\n",
        "           y_test, batch_size=batch_size, epochs=50, patience=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "18/18 [==============================] - 183s 4s/step - loss: 2.9650 - accuracy: 0.0536 - val_loss: 2.9460 - val_accuracy: 0.0543\n",
            "Epoch 2/50\n",
            "18/18 [==============================] - 34s 2s/step - loss: 2.9456 - accuracy: 0.0568 - val_loss: 2.9445 - val_accuracy: 0.0573\n",
            "Epoch 3/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9417 - accuracy: 0.0623 - val_loss: 2.9383 - val_accuracy: 0.0616\n",
            "Epoch 4/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9393 - accuracy: 0.0648 - val_loss: 2.9367 - val_accuracy: 0.0638\n",
            "Epoch 5/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9330 - accuracy: 0.0706 - val_loss: 2.9311 - val_accuracy: 0.0732\n",
            "Epoch 6/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9269 - accuracy: 0.0745 - val_loss: 2.9264 - val_accuracy: 0.0699\n",
            "Epoch 7/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9210 - accuracy: 0.0783 - val_loss: 2.9168 - val_accuracy: 0.0825\n",
            "Epoch 8/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9124 - accuracy: 0.0843 - val_loss: 2.9180 - val_accuracy: 0.0810\n",
            "Epoch 9/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.9074 - accuracy: 0.0867 - val_loss: 2.9031 - val_accuracy: 0.0876\n",
            "Epoch 10/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.8969 - accuracy: 0.0909 - val_loss: 2.8921 - val_accuracy: 0.0896\n",
            "Epoch 11/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.8836 - accuracy: 0.0959 - val_loss: 2.8767 - val_accuracy: 0.0976\n",
            "Epoch 12/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.8644 - accuracy: 0.1033 - val_loss: 2.8622 - val_accuracy: 0.1049\n",
            "Epoch 13/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.8368 - accuracy: 0.1152 - val_loss: 2.8158 - val_accuracy: 0.1211\n",
            "Epoch 14/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.7895 - accuracy: 0.1307 - val_loss: 2.7864 - val_accuracy: 0.1335\n",
            "Epoch 15/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.7391 - accuracy: 0.1449 - val_loss: 2.7048 - val_accuracy: 0.1557\n",
            "Epoch 16/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.6770 - accuracy: 0.1604 - val_loss: 2.6462 - val_accuracy: 0.1679\n",
            "Epoch 17/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.6335 - accuracy: 0.1716 - val_loss: 2.6115 - val_accuracy: 0.1770\n",
            "Epoch 18/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.5779 - accuracy: 0.1847 - val_loss: 2.5183 - val_accuracy: 0.2015\n",
            "Epoch 19/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.4943 - accuracy: 0.2047 - val_loss: 2.5530 - val_accuracy: 0.1870\n",
            "Epoch 20/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.4909 - accuracy: 0.2032 - val_loss: 2.4223 - val_accuracy: 0.2222\n",
            "Epoch 21/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.3941 - accuracy: 0.2277 - val_loss: 2.3391 - val_accuracy: 0.2407\n",
            "Epoch 22/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.3655 - accuracy: 0.2321 - val_loss: 2.3540 - val_accuracy: 0.2357\n",
            "Epoch 23/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.3957 - accuracy: 0.2246 - val_loss: 2.4510 - val_accuracy: 0.2144\n",
            "Epoch 24/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.3567 - accuracy: 0.2336 - val_loss: 2.2519 - val_accuracy: 0.2585\n",
            "Epoch 25/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.2212 - accuracy: 0.2622 - val_loss: 2.2552 - val_accuracy: 0.2513\n",
            "Epoch 26/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.2230 - accuracy: 0.2586 - val_loss: 2.1387 - val_accuracy: 0.2804\n",
            "Epoch 27/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.1167 - accuracy: 0.2831 - val_loss: 2.1084 - val_accuracy: 0.2814\n",
            "Epoch 28/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.0493 - accuracy: 0.2965 - val_loss: 2.0104 - val_accuracy: 0.3070\n",
            "Epoch 29/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 2.0290 - accuracy: 0.2986 - val_loss: 1.9920 - val_accuracy: 0.3100\n",
            "Epoch 30/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 1.9899 - accuracy: 0.3051 - val_loss: 1.9248 - val_accuracy: 0.3228\n",
            "Epoch 31/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 1.9606 - accuracy: 0.3122 - val_loss: 1.9397 - val_accuracy: 0.3144\n",
            "Epoch 32/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 1.8726 - accuracy: 0.3317 - val_loss: 1.9367 - val_accuracy: 0.3075\n",
            "Epoch 33/50\n",
            "18/18 [==============================] - 33s 2s/step - loss: 1.8320 - accuracy: 0.3407 - val_loss: 1.9286 - val_accuracy: 0.3176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD1hYuduSP1W"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "\n",
        "        ##################\n",
        "        # YOUR CODE HERE #\n",
        "        ##################\n",
        "\n",
        "\n",
        "    def call(self, query, values):\n",
        "\n",
        "\n",
        "        ##################\n",
        "        # YOUR CODE HERE #\n",
        "        ##################\n",
        "\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHRDRW5rS2uQ"
      },
      "source": [
        "class LuongGeneralAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongGeneralAttention, self).__init__()\n",
        "        \n",
        "        ##################\n",
        "        # YOUR CODE HERE #\n",
        "        ##################\n",
        "        \n",
        "        self.W\n",
        "\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
        "\n",
        "        ##################\n",
        "        # YOUR CODE HERE #\n",
        "        ##################\n",
        "\n",
        "\n",
        "\n",
        "# LUONGH example\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
        "\n",
        "        # LUONGH Dot-product\n",
        "        score = tf.transpose(tf.matmul(query_with_time_axis, \n",
        "                                       values_transposed), perm=[0, 2, 1])\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG2lKcqR7gyq"
      },
      "source": [
        "# Example of visualizatio of attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGja1hmv7gYT"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "annotations = np.transpose([[3,12,45], [59,2,5], [1,43,5], [4,3,45.3]])\n",
        "dec_hidden_state = [5,1,20]\n",
        "plt.figure(figsize=(1.5, 4.5))\n",
        "# Let's visualize our annotation (each column is an annotation)\n",
        "ax = sns.heatmap(annotations, annot=True, cmap=sns.light_palette(\"orange\", as_cmap=True), linewidths=1)\n",
        "# Let's visualize our decoder hidden state\n",
        "sns.heatmap(np.transpose(np.matrix(dec_hidden_state)), annot=True, cmap=sns.light_palette(\"purple\", as_cmap=True), linewidths=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUW9fn5w3tPU"
      },
      "source": [
        "# Report\n",
        "\n",
        "+ Bahdanau Attention and Luong General Attention implementation.\n",
        "+ Comparative.\n",
        "+ Weight visualization. \n",
        "+ Blog site.\n",
        "\n"
      ]
    }
  ]
}